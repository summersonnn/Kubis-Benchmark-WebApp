<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kubi's Benchmark Leaderboard</title>

    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com?plugins=typography"></script>

    <!-- Tabulator CSS -->
    <link href="https://unpkg.com/tabulator-tables@6.3.0/dist/css/tabulator_simple.min.css" rel="stylesheet">

    <!-- Custom styles -->
    <link href="css/styles.css" rel="stylesheet">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5Q9WND7R6K"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-5Q9WND7R6K');
</script>

<div class="flex h-screen overflow-hidden">
    <!-- Sidebar -->
    <aside class="w-72 bg-white border-r border-gray-200 flex flex-col z-10">
        <!-- Sidebar Header -->
        <div class="px-6 py-6 border-b border-gray-100">
            <h1 class="text-xl font-bold text-gray-900 leading-tight">Kubi's Benchmark</h1>
            <p class="text-xs text-gray-500 mt-2 font-medium">LLM Performance Leaderboard</p>
        </div>




        <!-- Home Link -->
        <div class="px-3 pt-4 pb-2">
            <button id="nav-home"
                class="w-full flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-700 hover:bg-gray-100 hover:text-gray-900 transition-colors group">
                <span class="mr-3 text-lg">üè†</span>
                Home
            </button>
            <button id="nav-configs"
                class="w-full flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-700 hover:bg-gray-100 hover:text-gray-900 transition-colors group mt-1">
                <span class="mr-3 text-lg">‚öôÔ∏è</span>
                Configs
            </button>
        </div>

        <!-- Run Selection -->
        <div class="flex-1 overflow-y-auto py-4">
            <div class="px-6 mb-3">
                <h2 class="text-xs font-semibold text-gray-400 uppercase tracking-wider">Benchmark Runs</h2>
            </div>
            <div id="run-list" class="space-y-1 px-3">
                <!-- Run items will be injected here -->
                <div class="px-3 py-2 text-sm text-gray-500 animate-pulse">Loading runs...</div>
            </div>
        </div>

        <!-- Sidebar Footer -->
        <div class="p-4 border-t border-gray-200 bg-gray-50">
            <div class="text-xs text-gray-400 text-center">
                &copy; 2026 Kubi's Benchmark
            </div>
        </div>
    </aside>

    <!-- Main Content Wrapper -->
    <main class="flex-1 flex flex-col min-w-0 bg-gray-50 overflow-hidden">

        <!-- Page: Home -->
        <div id="page-home" class="hidden flex-1 flex flex-col min-h-0 overflow-auto">
            <div class="p-8 max-w-4xl mx-auto">
                <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-8 mb-8">
                    <h1 class="text-3xl font-bold text-gray-900 mb-6">Welcome to Kubi's Benchmark</h1>

                    <div class="prose prose-indigo max-w-none text-gray-600 mb-8">
                        <p><strong>Hi,</strong></p>
                        <p>
                            There are plenty of benchmarks out there, and I understand why many people are cautious
                            about them.
                            I shared that skepticism, which is why I decided to build one myself. Everything here from
                            the questions
                            to the evaluation scripts was created from scratch by me (with some help from Claude of
                            course).
                            While the internet influenced some question ideas, nothing was directly reused. I plan to
                            rerun this benchmark on the <b>first day of each month</b>, testing only newly released
                            models and
                            questions and replacing the published questions with new ones. Any major model release will
                            be evaluated as soon as possible.
                        </p>
                    </div>

                    <!-- Countdown Section -->
                    <div
                        class="mb-10 p-6 bg-gradient-to-r from-gray-50 to-white rounded-xl border border-gray-100 text-center">
                        <div class="text-sm font-semibold text-gray-500 uppercase tracking-wider mb-2">Remaining time to
                            next run</div>
                        <div id="countdown-timer"
                            class="text-5xl font-mono font-bold text-indigo-600 tracking-tight mb-1">
                            --:--:--:--
                        </div>
                        <div class="text-xs text-gray-400 italic">(Or next big model release)</div>
                    </div>

                    <!-- Status Cards -->
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-12">
                        <div class="bg-red-50 rounded-lg p-6 border border-red-100">
                            <h3 class="font-semibold text-red-900 mb-2">The "Bad" Stuff</h3>
                            <p class="text-red-800 text-sm leading-relaxed">
                                This benchmark does not currently include a coding category. I first added coding
                                questions and set up
                                an evaluation pipeline, but the scoring had to be done manually and took a huge amount
                                of time even
                                for one model, so I ended up removing it. All remaining questions are evaluated
                                automatically.
                            </p>
                        </div>
                        <div class="bg-indigo-50 rounded-lg p-6 border border-indigo-100">
                            <h3 class="font-semibold text-indigo-900 mb-2">The Exciting Stuff</h3>
                            <p class="text-indigo-800 text-sm leading-relaxed">
                                I am working on a separate project focused entirely on benchmarking models through
                                coding game agents.
                                It will be competitive, with models playing against each other, and should be much more
                                engaging.
                                That will be released later, probably next week.
                            </p>
                        </div>
                    </div>

                    <h2 class="text-2xl font-bold text-gray-900 mb-8 border-b border-gray-100 pb-4">What Sets It Apart
                    </h2>

                    <div class="space-y-8">
                        <!-- 1 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                1</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Mix of X instead of Best of X</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    Many benchmarks generate multiple outputs per question and mark the result as a pass
                                    if any one output is correct ("best of X").
                                    Here, scores are averaged across all runs. For example, if a question is worth 5
                                    points and four runs score 5, 0, 0, and 4,
                                    the final score for that question is 9/4 = 2.25.
                                </p>
                            </div>
                        </div>

                        <!-- 2 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                2</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Two evaluation methods</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    Questions are evaluated either by a judge LLM or by a custom verifier script. The
                                    judge LLM (Gemini 3.0 Flash in my case)
                                    has access to the ground truth and marks answers as pass or fail. Verifier scripts
                                    are written specifically for individual
                                    questions and programmatically check the model's output.
                                </p>
                            </div>
                        </div>

                        <!-- 3 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                3</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Partial credit</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    Some questions support partial points, but only when evaluated by verifier scripts.
                                    I don't rely on judge LLMs for partial scoring.
                                    With script-based verification, partial credit has been reliable.
                                </p>
                            </div>
                        </div>

                        <!-- 4 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                4</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Token limits tied to question value</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    Each question has a point value, and the maximum token limit scales with it. A
                                    1-point question uses a base limit of 8,196 tokens,
                                    while a 5-point question allows up to roughly 40k tokens. Harder questions are given
                                    more room for reasoning.
                                </p>
                            </div>
                        </div>

                        <!-- 5 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                5</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Gradual release of questions</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    The repository is open source, but the full question set is not publicly available
                                    yet. This is to avoid future models training directly on the benchmark.
                                    Instead, I will release questions worth about 10% of the total points each month
                                    when I run new evaluations and replace them with new questions.
                                    The first batch is already published on the website.
                                </p>
                            </div>
                        </div>

                        <!-- 6 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                6</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Dynamic point adjustment</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    After initial runs, I noticed that some questions were misweighted. To reduce
                                    personal bias, I introduced an automatic adjustment system.
                                    If all models fully solve a question, its point value is reduced. If none succeed,
                                    the value increases. Intermediate outcomes are adjusted proportionally.
                                    A secondary leaderboard based on this dynamic scoring is also available.
                                </p>
                            </div>
                        </div>

                        <!-- 7 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                7</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Controlled model and provider selection
                                </h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    OpenRouter models are used with at least FP8 quantization for open-source models.
                                    Providers were selected based on accumulated community feedback
                                    and broader observations. Certain providers were excluded due to consistently poor
                                    API performance.
                                </p>
                            </div>
                        </div>

                        <!-- 8 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                8</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Varied and original questions</h3>
                                <ul class="mt-2 text-gray-600 list-disc list-inside space-y-1 ml-1">
                                    <li><strong>Basic Mix:</strong> Simple tasks or altered well-known questions to test
                                        overfitting.</li>
                                    <li><strong>General Knowledge:</strong> Deep knowledge checks and "future
                                        prediction" questions (events that happened after model cutoff).</li>
                                    <li><strong>Math:</strong> Medium to hard problems sourced from private sources.
                                    </li>
                                    <li><strong>Reasoning:</strong> Logic and puzzle-based questions, including chess
                                        and word puzzles.</li>
                                </ul>
                            </div>
                        </div>

                        <!-- 9 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                9</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">Broad model coverage</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    The benchmark includes leading proprietary models, strong open-source options, and
                                    models that can realistically run on consumer GPUs.
                                    I'm open to suggestions for missing models.
                                </p>
                            </div>
                        </div>

                        <!-- 10 -->
                        <div class="flex gap-4">
                            <div
                                class="flex-shrink-0 w-8 h-8 flex items-center justify-center rounded-full bg-indigo-100 text-indigo-600 font-bold text-sm">
                                10</div>
                            <div>
                                <h3 class="text-lg font-semibold text-gray-900">High reasoning effort</h3>
                                <p class="mt-2 text-gray-600 leading-relaxed">
                                    All requests are sent with reasoning effort set to high, where supported by the
                                    model.
                                </p>
                            </div>
                        </div>
                    </div>

                    <!-- Footer Note -->
                    <div class="mt-12 pt-8 border-t border-gray-100 text-center">
                        <p class="text-lg font-medium text-indigo-600 animate-pulse">
                            Lastly, wait for the more exciting competitive game agent coding benchmark league that will
                            be published by me soon!
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Page: Configs -->
        <div id="page-configs" class="hidden flex-1 flex flex-col min-h-0 overflow-auto">
            <div class="p-8 max-w-4xl mx-auto">
                <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-8 mb-8">
                    <h1 class="text-3xl font-bold text-gray-900 mb-6">Configuration</h1>

                    <h1 class="text-3xl font-bold text-gray-900 mb-6">Configuration</h1>

                    <p class="text-gray-600 mb-8">These are OpenRouter settings, used for the benchmark.</p>

                    <div class="prose prose-indigo max-w-none text-gray-600 mb-8">
                        <h2 class="text-xl font-bold text-gray-800 mb-4">Models Used</h2>
                        <ul
                            class="list-none space-y-2 font-mono text-sm bg-gray-50 p-4 rounded-lg border border-gray-100">
                            <li>google/gemini-3-flash-preview</li>
                            <li>google/gemini-3-pro-preview</li>
                            <li>deepseek/deepseek-v3.2@preset/fp8</li>
                            <li>openai/gpt-5.2</li>
                            <li>qwen/qwen3-max</li>
                            <li>openai/gpt-5-mini</li>
                            <li>anthropic/claude-opus-4.5</li>
                            <li>anthropic/claude-sonnet-4.5</li>
                            <li>anthropic/claude-haiku-4.5</li>
                            <li>x-ai/grok-4</li>
                            <li>x-ai/grok-4.1-fast</li>
                            <li>openai/gpt-oss-120b@preset/fp8</li>
                            <li>z-ai/glm-4.7@preset/fp8-speedy</li>
                            <li>z-ai/glm-4.7-flash@preset/fp8-speedy</li>
                            <li>moonshotai/kimi-k2.5</li>
                            <li>moonshotai/kimi-k2-thinking</li>
                            <li>nvidia/nemotron-3-nano-30b-a3b@preset/fp8</li>
                            <li>meta-llama/llama-4-scout@preset/fp8</li>
                            <li>minimax/minimax-m2.1@preset/fp8</li>
                            <li>qwen/qwen3-235b-a22b-thinking-2507@preset/fp8</li>
                            <li>qwen/qwen3-next-80b-a3b-thinking@preset/fp8</li>
                            <li>qwen/qwen3-32b@preset/fp8</li>
                            <li>xiaomi/mimo-v2-flash</li>
                            <li>google/gemma-3-27b-it@preset/fp8</li>
                            <li>mistralai/mistral-large-2512</li>
                        </ul>
                    </div>

                    <div class="prose prose-indigo max-w-none text-gray-600 mb-8">
                        <h2 class="text-xl font-bold text-gray-800 mb-4">Preset Configs</h2>

                        <h3 class="font-semibold text-gray-700">Provider Preferences</h3>
                        <pre class="bg-gray-900 text-gray-100 rounded-lg p-4 font-mono text-xs overflow-x-auto">
{
  "sort": {
    "by": "price",
    "partition": null
  },
  "quantizations": [
    "fp8",
    "fp16",
    "bf16"
  ],
  "allow_fallbacks": true,
  "data_collection": "allow"
}</pre>

                        <h3 class="font-semibold text-gray-700 mt-6">Parameters</h3>
                        <pre class="bg-gray-900 text-gray-100 rounded-lg p-4 font-mono text-xs overflow-x-auto">
{
  "reasoning": {
    "effort": "high",
    "enabled": true
  }
}</pre>

                        <div class="mt-4 p-4 bg-indigo-50 border border-indigo-100 rounded-lg text-sm text-indigo-800">
                            <strong>Note:</strong> fp8+speedy is exactly same as fp8 with only difference is most
                            fastest provider is selected instead of the cheapest. (So, "by": "throughput")
                        </div>
                    </div>

                    <div class="prose prose-indigo max-w-none text-gray-600">
                        <h2 class="text-xl font-bold text-gray-800 mb-4">Provider Selection Settings</h2>
                        <img src="/data/media/openrouter_global_settings.png" alt="Provider Restrictions Settings"
                            class="rounded-lg border border-gray-200 shadow-sm mb-4">
                        <p class="text-sm italic">
                            Justification: Some providers are known to be less performant than others, so I excluded
                            them.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Page: Results -->
        <div id="page-results" class="flex-1 flex flex-col min-h-0">
            <!-- Top Bar / Header for Mobile or Context -->
            <header
                class="bg-white shadow-sm border-b border-gray-200 px-8 py-4 flex justify-between items-center z-10">
                <div>
                    <h2 class="text-lg font-semibold text-gray-800" id="current-run-title">Select a Run</h2>
                    <p class="text-sm text-gray-500" id="run-date">Loading...</p>
                </div>
                <!-- Additional controls could go here -->
            </header>

            <!-- Scrollable Content Area -->
            <div class="flex-1 overflow-auto p-8">
                <div class="max-w-7xl mx-auto">
                    <!-- Tab Navigation -->
                    <div class="border-b border-gray-200 mb-6">
                        <nav class="flex space-x-8" aria-label="Tabs">
                            <button class="tab-btn active" data-tab="rankings">
                                Model Rankings v1
                            </button>
                            <button class="tab-btn" data-tab="details">
                                Detailed Results
                            </button>
                            <button class="tab-btn" data-tab="chart">
                                Chart
                            </button>
                            <button class="tab-btn" data-tab="rankings_v2">
                                Rankings v2
                            </button>
                        </nav>
                    </div>

                    <!-- Tab: Model Rankings -->
                    <section id="tab-rankings" class="tab-content active">
                        <div class="bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                            <div id="summary-table"></div>
                        </div>
                    </section>

                    <!-- Tab: Detailed Results -->
                    <section id="tab-details" class="tab-content hidden">
                        <div class="bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                            <div id="detail-table-top-scrollbar" class="top-scrollbar-container">
                                <div class="top-scrollbar-dummy"></div>
                            </div>
                            <div id="detail-table"></div>
                        </div>
                    </section>

                    <!-- Tab: Chart -->
                    <section id="tab-chart" class="tab-content hidden">
                        <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-6">
                            <div class="flex items-center gap-4 mb-4">
                                <label for="chart-category" class="text-sm font-medium text-gray-700">Category:</label>
                                <select id="chart-category"
                                    class="rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 text-sm px-3 py-2 border">
                                    <option value="General" selected>General (Total)</option>
                                    <option value="Reasoning">Reasoning</option>
                                    <option value="General Knowledge">General Knowledge</option>
                                    <option value="Math">Math</option>
                                    <option value="Basic Mix">Basic Mix</option>
                                </select>
                            </div>
                            <div style="height: 400px; position: relative;">
                                <canvas id="score-chart"></canvas>
                            </div>
                        </div>
                    </section>

                    <!-- Tab: Rankings v2 -->
                    <section id="tab-rankings_v2" class="tab-content hidden">
                        <div class="bg-white rounded-xl shadow-sm border border-gray-200 p-8 mb-6">
                            <div class="prose prose-indigo max-w-none text-gray-600">
                                <h3 class="text-xl font-bold text-gray-900 mb-4">Dynamic Point Adjustment</h3>
                                <p>
                                    Initial evaluations revealed that certain questions were inconsistently weighted. To
                                    minimize subjective bias,
                                    I implemented an automated adjustment mechanism: point values are dynamically
                                    reduced if a question is solved
                                    by all models, and increased if none succeed. Intermediate results are adjusted
                                    proportionally. This tab presents
                                    a secondary leaderboard based on this dynamic scoring system.
                                </p>
                            </div>
                        </div>
                        <div class="bg-white rounded-xl shadow-sm border border-gray-200 overflow-hidden">
                            <div id="summary-table-v2"></div>
                        </div>
                    </section>
                </div>
            </div>
        </div><!-- End page-results -->

        <!-- Page: Published Questions -->
        <div id="page-published" class="hidden flex-1 flex flex-col min-h-0">
            <header
                class="bg-white shadow-sm border-b border-gray-200 px-8 py-4 flex justify-between items-center z-10">
                <div>
                    <h2 class="text-lg font-semibold text-gray-800">Published Questions</h2>
                    <p class="text-sm text-gray-500">View the library of benchmark questions</p>
                </div>
            </header>

            <div class="flex-1 overflow-auto p-8">
                <div class="max-w-7xl mx-auto">
                    <div id="questions-content" class="bg-white rounded-xl shadow-sm border border-gray-200 p-12">
                        <div class="text-center">
                            <div class="text-6xl mb-4">üìö</div>
                            <h3 class="text-xl font-medium text-gray-900 mb-2">Question Library</h3>
                            <p class="text-gray-500 max-w-lg mx-auto">
                                This page will contain the full list of questions used in the benchmark, including
                                prompt
                                details, reference answers, and judging criteria.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>
</div>

<!-- Footer -->
<footer class="bg-white border-t border-gray-200 mt-12">
    <div class="max-w-7xl mx-auto px-4 py-6 text-center text-gray-500 text-sm">
        <p>Kubi's Benchmark - Evaluating LLM capabilities</p>
    </div>
</footer>

<!-- Tabulator JS -->
<script type="text/javascript" src="https://unpkg.com/tabulator-tables@6.3.0/dist/js/tabulator.min.js"></script>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>

<!-- Marked.js for Markdown parsing -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Main application JS -->
<script src="js/main.js?v=15"></script>
</body>

</html>