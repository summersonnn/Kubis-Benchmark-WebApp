# Kubi's Benchmark Leaderboard - Project Documentation

## Overview

A static website for displaying LLM benchmark results. The site parses benchmark HTML files generated by an external benchmark runner and presents the data in sortable tables and charts.

## Tech Stack

| Technology | Purpose | CDN |
|------------|---------|-----|
| **Tailwind CSS** | Utility-first styling | `https://cdn.tailwindcss.com` |
| **Tabulator.js 6.3.0** | Interactive sortable/grouped tables | `https://unpkg.com/tabulator-tables@6.3.0` |
| **Chart.js 4.4.1** | Bar chart visualization | `https://cdn.jsdelivr.net/npm/chart.js@4.4.1` |
| **Vanilla JavaScript** | HTML parsing, application logic | N/A |

No build step required. All dependencies loaded via CDN.

## Project Structure

```
/
├── index.html                 # Main page with tabs and table containers
├── css/
│   └── styles.css             # Custom styles (medals, badges, Tabulator overrides)
├── js/
│   └── main.js                # Core application logic
├── data/
│   ├── runs.json              # Manifest listing available benchmark files
│   └── runs/                  # Directory containing benchmark HTML files
│       ├── performance_table_mock_extended.html
│       ├── performance_table_mock.html
│       └── performance_table_*.html
└── CLAUDE.md                  # This documentation file
```

## Data Format

### Benchmark HTML Structure

The site parses HTML files generated by the benchmark runner. Required structure:

```html
<table>
    <thead>
        <tr>
            <th rowspan="2">Question Index</th>
            <th rowspan="2">Points</th>
            <th colspan="3" class="model-header">model-name-1</th>
            <th colspan="3" class="model-header">model-name-2</th>
            <!-- More models... -->
        </tr>
        <tr>
            <th>Score</th><th>Tokens</th><th>Cost</th>
            <!-- Repeated for each model -->
        </tr>
    </thead>
    <tbody>
        <tr data-category="Category Name" data-subcategory="Subcategory Name">
            <td class="q-col">A1-J-question-description</td>
            <td>2</td>  <!-- Points for this question -->
            <td class="pass">PASS</td>  <!-- or "FAIL" or numeric like "0.5" -->
            <td class="tokens">1245</td>
            <td class="cost">$0.02</td>
            <!-- Repeated for each model -->
        </tr>
        <!-- More question rows... -->
    </tbody>
    <tfoot>
        <tr>
            <td colspan="2">TOTAL</td>
            <td class="score">36.75/41</td>
            <td class="tokens">27237</td>
            <td class="cost">$0.41</td>
            <!-- Repeated for each model -->
        </tr>
    </tfoot>
</table>
```

### Critical Data Attributes

- `data-category`: Category name (e.g., "Coding", "Reasoning", "STEM")
- `data-subcategory`: Subcategory name (e.g., "Leetcode", "Chess puzzle") - can be empty
- `class="model-header"`: Identifies model name headers
- `class="pass"` / `class="fail"`: Score cell classes (optional, text content is authoritative)

### Categories

The following categories are supported (in display order):

1. **Basic Mix**
   - Anti-Overfitting
   - Terminal Usage
   - Others

2. **Coding**
   - Leetcode
   - Competitive Gaming
   - Single HTML

3. **General Knowledge** (no subcategories)

4. **Reasoning**
   - Chess puzzle
   - Word puzzle
   - Pure Reasoning

5. **STEM**
   - Physics
   - Chemistry
   - Biology
   - Math

### Runs Manifest (`data/runs.json`)

```json
{
  "runs": [
    {
      "file": "performance_table_mock_extended.html",
      "date": "2026-01-21 16:00:00 (Extended Mock - 8 Models)"
    }
  ]
}
```

Update this file when adding new benchmark runs.

## Score Calculation

Scores are calculated consistently across all views (Model Rankings, Detailed Results, Chart):

| Score Type | Calculation |
|------------|-------------|
| **PASS** | Full question points (e.g., 2 points for a 2-point question) |
| **FAIL** | 0 points |
| **Numeric** (e.g., 0.5, 0.67) | Absolute value (0.5 means 0.5 points, NOT 50%) |

**Important**: The site calculates totals from individual question scores, NOT from the HTML footer totals. This ensures consistency across all views.

### Display Format

- Scores are displayed as `score/max` (e.g., "36.75/41")
- Scores are limited to 2 decimal places
- Category scores (Coding, Reasoning, STEM) shown in Model Rankings table

## Features

### 1. Model Rankings Tab

Main leaderboard table with columns:
- **Rank**: Auto-calculated with medal icons for top 3 (gold/silver/bronze)
- **Model**: Model name
- **Score**: Total score as `earned/max`
- **Total Tokens**: Sum of all tokens used
- **Total Cost**: Sum of all costs
- **Coding**: Category score
- **Reasoning**: Category score
- **STEM**: Category score

All columns are sortable. Models sorted by score (highest first) by default.

### 2. Detailed Results Tab

Grouped table showing per-question results:
- Grouped by Category > Subcategory (collapsible)
- Shows Question index (e.g., "A1", "A9"), Points
- Per-model columns: Score, Tokens, Cost
- Category totals at bottom of each category
- Grand total at bottom

Models are ordered left-to-right by score (highest on left).

Vertical separators between model sections for visual clarity.

### 3. Chart Tab

Bar chart visualization with category filter:
- **General (Total)**: Shows overall scores
- **Coding**: Shows only Coding category scores
- **Reasoning**: Shows only Reasoning category scores
- **STEM**: Shows only STEM category scores

Chart updates automatically when category is changed.

### 4. Run Selector

Dropdown to switch between different benchmark runs. Located in the controls section below the header.

## Styling

### Score Badges

| Class | Appearance | Usage |
|-------|------------|-------|
| `.score-pass` | Green badge | PASS results |
| `.score-fail` | Red badge | FAIL results |
| `.score-partial` | Yellow/amber badge | Partial scores |
| `.score-total` | Blue badge | Category/grand totals |

### Rank Medals

| Class | Appearance |
|-------|------------|
| `.rank-gold` | Gold gradient, rank 1 |
| `.rank-silver` | Silver gradient, rank 2 |
| `.rank-bronze` | Bronze gradient, rank 3 |
| `.rank-number` | Gray text, rank 4+ |

### Table Customizations

- No row separators (clean look)
- Vertical separators between model sections in detailed view
- Alternating row colors (white/light gray)
- Sticky headers
- Responsive with horizontal scroll on mobile

## Adding New Benchmark Runs

1. Generate benchmark HTML with your benchmark runner
2. Ensure HTML includes `data-category` and `data-subcategory` attributes on each `<tr>`
3. Place HTML file in `data/runs/` directory
4. Update `data/runs.json` manifest with new entry
5. Refresh the page

## Key Functions in `main.js`

| Function | Purpose |
|----------|---------|
| `init()` | Application entry point |
| `loadManifest()` | Fetches `runs.json` |
| `loadRun(filename)` | Loads and parses a benchmark HTML file |
| `parseHTML(html)` | Extracts models, questions, and results from HTML |
| `renderSummaryTable(data)` | Renders Model Rankings tab |
| `renderDetailTable(data)` | Renders Detailed Results tab |
| `renderChart(data)` | Renders Chart tab with category filtering |
| `scoreFormatter(cell)` | Formats score cells with appropriate badges |
| `rankFormatter(cell)` | Formats rank with medals |
| `formatScore(score)` | Limits score to 2 decimal places |

## Hosting

Designed for static hosting (GitHub Pages, Netlify, etc.). No server-side processing required.

Since JavaScript cannot scan directories on static hosting, the `data/runs.json` manifest must be manually updated when adding new benchmark files.

## Browser Support

Modern browsers with ES6+ support. Tested on:
- Chrome/Chromium
- Firefox
- Safari
- Edge

## Development

To run locally:

```bash
cd /path/to/project
python -m http.server 8000
# Open http://localhost:8000 in browser
```

A local HTTP server is required due to CORS restrictions when fetching local files.
